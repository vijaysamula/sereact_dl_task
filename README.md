# Distributed AI Inference System

A microservice-based AI inference system that simulates distributed processing with multiple worker nodes, fault tolerance, and load balancing.

## ğŸ¯ Assignment Requirements Met

âœ… **Microservice Architecture**: Coordinator + 3 Worker services  
âœ… **Multiple Models**: DistilBERT, MobileNetV2, CLIP  
âœ… **Fault Tolerance**: Simulated failures, retries, health checks  
âœ… **Batching & Queuing**: Task queue with batch processing  
âœ… **Logging & Monitoring**: Detailed task logs + CLI monitor  
âœ… **Docker Compose**: Full containerization  
âœ… **Async I/O**: FastAPI with asyncio  
âœ… **Burst Traffic Test**: Per-request success/failure reporting  

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- 4GB+ RAM (for model loading)

### 1. Start the System
```bash
git clone https://github.com/vijaysamula/sereact_dl_task.git
cd sereact_dl_task

# Start all services
docker-compose up --build

# Wait 3-5 minutes for models to load
# Look for "Models loaded successfully" messages
```

### 2. Test Basic Functionality
```bash
# Test text classification
curl -X POST "http://localhost:8000/infer" \
  -H "Content-Type: application/json" \
  -d '{
    "request_id": "test-1",
    "task_type": "text_classification",
    "data": "I love this product!"
  }'

# Test image classification
curl -X POST "http://localhost:8000/infer" \
  -H "Content-Type: application/json" \
  -d '{
    "request_id": "test-2", 
    "task_type": "image_classification",
    "data": "dummy_image_data"
  }'
```

### 3. Monitor the System
```bash
# Real-time CLI monitor
python monitor_cli.py

# Check system status
curl http://localhost:8000/status
```

### 4. Run Burst Traffic Test (Assignment Requirement)
```bash
# Burst traffic test with per-request success/failure reporting
python burst_traffic_test.py --pattern mixed

# Simple load test
python simple_load_test.py
```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/infer` | POST | Submit inference request |
| `/status` | GET | System status summary |
| `/queue/stats` | GET | Queue and worker statistics |
| `/workers/performance` | GET | Worker performance metrics |

## ğŸ§ª Supported Task Types

- **text_classification**: Sentiment analysis using DistilBERT
- **image_classification**: Object recognition using MobileNetV2  
- **clip_text_similarity**: Text similarity using CLIP
- **clip_text_to_image**: Text-to-image matching using CLIP

## ğŸ—ï¸ Architecture

```
Client â†’ Coordinator â†’ Worker Pool
                    â”œâ”€â”€ Worker 1 (DistilBERT)
                    â”œâ”€â”€ Worker 2 (MobileNetV2)  
                    â””â”€â”€ Worker 3 (CLIP)
```

## ğŸ”§ Failure Simulation

Configure via environment variables:
- `CRASH_PROBABILITY=0.02` (2% crash rate)
- `TIMEOUT_PROBABILITY=0.03` (3% timeout rate)
- `NETWORK_DELAY_MIN=0.1` (Min network delay)
- `NETWORK_DELAY_MAX=0.8` (Max network delay)

## ğŸ“ Key Files

```
â”œâ”€â”€ docker-compose.yml          # Multi-service orchestration
â”œâ”€â”€ coordinator/
â”‚   â”œâ”€â”€ coordinator.py          # Main coordinator logic
â”‚   â””â”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ worker.py              # Worker implementation
â”‚   â””â”€â”€ main.py                # Worker FastAPI app
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ models.py              # Data models
â”‚   â””â”€â”€ logging_setup.py       # Logging setup
â”œâ”€â”€ monitor_cli.py             # Real-time monitor
â”œâ”€â”€ burst_traffic_test.py      # Assignment requirement
â””â”€â”€ simple_load_test.py        # Basic load test
```

## ğŸš¨ Testing Scenarios

### 1. Normal Operation
```bash
python simple_load_test.py
```

### 2. Burst Traffic (Assignment Requirement)
```bash
python burst_traffic_test.py --pattern burst
```

### 3. Failure Simulation
```bash
# Stop a worker to test fault tolerance
docker-compose stop worker-1

# System automatically routes to other workers
```

## ğŸ› ï¸ Troubleshooting

**Models not loading?**
```bash
# Check worker logs
docker-compose logs worker-1

# Ensure sufficient memory
docker stats
```

**Connection errors?**
```bash
# Wait for full startup (3-5 minutes)
curl http://localhost:8000/status
```

**High latency?**
```bash
# Monitor real-time
python monitor_cli.py
```

## ğŸ“ˆ Expected Performance

- **Text Classification**: 0.2-0.6s
- **Image Classification**: 0.5-1.0s
- **CLIP Tasks**: 0.8-1.5s
- **Success Rate**: >95% under normal load
- **Automatic Retries**: Up to 3 attempts
- **Health Checks**: Every 10 seconds

## ğŸ“ Assignment Compliance

This implementation fulfills all assignment requirements:

1. âœ… **Microservice Architecture**: REST-based coordinator + 3 workers
2. âœ… **Fault Tolerance**: Simulated failures, retries, health monitoring
3. âœ… **Batching & Queuing**: Priority queue with efficient batching
4. âœ… **Logging**: Detailed task logs with metadata
5. âœ… **Model Usage**: DistilBERT, MobileNetV2, CLIP models
6. âœ… **Docker Compose**: Full containerization
7. âœ… **Async I/O**: FastAPI throughout
8. âœ… **Burst Traffic Test**: Per-request success/failure reporting
9. âœ… **Live Monitoring**: CLI dashboard

## ğŸ™ Notes

- Models load in ~3-5 minutes on first startup
- Each worker supports all task types for better load distribution
- System automatically handles worker failures with retries
- All logs stored in `logs/task_operations.log`
- Monitor provides real-time system visibility